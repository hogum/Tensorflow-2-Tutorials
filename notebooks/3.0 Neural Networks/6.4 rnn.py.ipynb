{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character based recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1056768/1049619 [==============================] - 11s 11us/step\n"
     ]
    }
   ],
   "source": [
    "file_url = 'https://www.gutenberg.org/files/1400/1400-0.txt'\n",
    "file_path = tf.keras.utils.get_file('1400-0.txt', file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(file_path).read()\n",
    "# Strip off instruction text\n",
    "text = text[824:18781]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 61\n",
      " ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "# get unique characters\n",
    "vocab = sorted(set(text))\n",
    "print(f'Unique characters: {len(vocab)}\\n {vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign index values to each character\n",
    "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the text to array of integers\n",
    "\n",
    "text_as_int = np.asarray([char_to_index[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17957,)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name being Pirrip, a\n",
      "n:46 a:33 m:45 e:37  :1 b:34 e:37 i:41 n:46 g:39  :1 P:26 i:41 r:50 r:50 i:41 p:48 ,:6  :1 a:33 "
     ]
    }
   ],
   "source": [
    "# show character mapping\n",
    "\n",
    "start, stop = 30, 50\n",
    "\n",
    "print(text[start:stop])\n",
    "for i in range(start, stop):\n",
    "    print(f'{text[i]}:{text_as_int[i]}', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare an input dataset from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name"
     ]
    }
   ],
   "source": [
    "# Create input Dataset\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for idx in char_dataset.take(70):\n",
    "    print(idx_to_char[idx], end= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data for training\n",
    "\n",
    "sequence_len = 80\n",
    "examples_per_epoch = len(text) / sequence_len\n",
    "\n",
    "sequences = char_dataset.batch(batch_size=sequence_len + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(text_chunk):\n",
    "    \"\"\"\n",
    "        Creates the input and target data by shifting one\n",
    "        character to the right\n",
    "        \n",
    "        Example: Outside -> utsider\n",
    "    \"\"\"\n",
    "    input_txt = text_chunk[:-1]\n",
    "    target_txt = text_chunk[1:]\n",
    "    \n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display the input and target data\n",
    "\n",
    "- `dataset.take(n)`returns `n` batches.\n",
    "- The batch size = `sequence_len` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  Chapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name Philip, m\n",
      "Target:  hapter I\n",
      "\n",
      "My father's family name being Pirrip, and my Christian name Philip, my\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "# display for 2 batches\n",
    "input_example, target_example = [], []\n",
    "\n",
    "for input, target in dataset.take(1):\n",
    "    print(r'Input: ', ''.join(idx_to_char[input.numpy()]))\n",
    "    print(r'Target: ', ''.join(idx_to_char[target.numpy()]))\n",
    "    print('----------------')\n",
    "    \n",
    "    input_example.append(input.numpy())\n",
    "    target_example.append(target.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display input and the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n",
      "\n",
      "input: C (14)\n",
      "target:  h (40)\n",
      "step: 1\n",
      "\n",
      "input: h (40)\n",
      "target:  a (33)\n",
      "step: 2\n",
      "\n",
      "input: a (33)\n",
      "target:  p (48)\n",
      "step: 3\n",
      "\n",
      "input: p (48)\n",
      "target:  t (52)\n",
      "step: 4\n",
      "\n",
      "input: t (52)\n",
      "target:  e (37)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[0][:5], target_example[0][:5])):\n",
    "    print(f'step: {i}\\n')\n",
    "    print(f'input: {idx_to_char[input_idx]} ({input_idx})')\n",
    "    print(f'target:  {idx_to_char[target_idx]} ({target_idx})')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # chars in batch\n",
    "steps_per_epoch = examples_per_epoch // batch_size\n",
    "buffer_size = text_as_int.size\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# re-feed data to the model from the beginning\n",
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model has the layers:\n",
    "\n",
    "    a) Embedding layers - Lookup table of vectors\n",
    "    \n",
    "    b) Gate Recurrent Unit\n",
    "    \n",
    "    c) Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "embedding_dimension = 256\n",
    "recurrent_nn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    recurrent_nn = tf.keras.layers.CuDNNGRU\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    from functools import partial\n",
    "    recurrent_nn = partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(batch_size=64):\n",
    "    layers = [tf.keras.layers.Embedding(input_dim=vocab_len,\n",
    "                                        output_dim=embedding_dimension,\n",
    "                                        batch_input_shape=[batch_size, None]\n",
    "                                       ),\n",
    "              recurrent_nn(units=recurrent_nn_units,\n",
    "                          return_sequences=True,\n",
    "                          stateful=True),\n",
    "              tf.keras.layers.Dense(vocab_len)\n",
    "             ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7efe0032b6d0>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (64, 80, 61)   # [Batch, sequence_len, vocab_len]\n"
     ]
    }
   ],
   "source": [
    "# Check output shape of model\n",
    "batch_input_ex, batch_target_ex = [], []\n",
    "\n",
    "for input_batch, target_batch in dataset.take(1):\n",
    "    batch_pred = model(input_batch)\n",
    "    batch_input_ex.append(input_batch)\n",
    "    batch_target_ex.append(target_batch)\n",
    "    \n",
    "    print(f'Output shape: {batch_pred.shape}   # [Batch, sequence_len, vocab_len]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (64, None, 256)           15616     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (64, None, 61)            62525     \n",
      "=================================================================\n",
      "Total params: 4,016,445\n",
      "Trainable params: 4,016,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get model predictions\n",
    "- This is achieved by **sampling** the output distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(logits=batch_pred[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([80, 1])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:   ay of his fast\n",
      "diminishing slice, to enter upon our usual friendly competition; \n",
      "\n",
      "Output:  Dnde“dTacOgWMo\n",
      "ikKxP“\n",
      "jYMjOgT.yei;QpobF\n",
      "MnqKzKhxO”wNJDx!cIe-k(H)Yx;uQG?.;vr“C BT\n"
     ]
    }
   ],
   "source": [
    "# input and output before training\n",
    "\n",
    "input_ = ''.join(idx_to_char[batch_input_ex[0][0].numpy()])\n",
    "output_ = ''.join(idx_to_char[sampled_indices])\n",
    "\n",
    "print('Input:  ', input_)\n",
    "print()\n",
    "print('Output: ', output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar loss: 4.109776496887207\n"
     ]
    }
   ],
   "source": [
    "# Loss before training\n",
    "\n",
    "batch_loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(batch_target_ex[0].numpy(), batch_pred)\n",
    "print(f'Scalar loss: {batch_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Train for longer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 13s 4s/step - loss: 3.0133\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 11s 4s/step - loss: 2.9072\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 11s 4s/step - loss: 2.8100\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 13s 4s/step - loss: 2.7154\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 11s 4s/step - loss: 2.6327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efdf1ca9550>"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '.model.rnn'\n",
    "epochs = 100\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=loss_f)\n",
    "\n",
    "# save checkpoints\n",
    "file_prefix = os.path.join(model_path, 'ckpt_{epoch}')\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath=file_prefix, save_weights_only=True)\n",
    "\n",
    "# Train\n",
    "model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest checkpoint:  .model.rnn/ckpt_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0901 23:21:44.214857 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
      "W0901 23:21:44.215723 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W0901 23:21:44.216896 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W0901 23:21:44.217730 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W0901 23:21:44.219081 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W0901 23:21:44.221703 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W0901 23:21:44.223446 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "W0901 23:21:44.224956 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "W0901 23:21:44.226416 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "W0901 23:21:44.227770 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "W0901 23:21:44.229123 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "W0901 23:21:44.230444 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "W0901 23:21:44.231833 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "W0901 23:21:44.233375 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "W0901 23:21:44.234924 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "W0901 23:21:44.236272 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "W0901 23:21:44.237843 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "W0901 23:21:44.241815 139632326494016 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "W0901 23:21:44.253675 139632326494016 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (1, None, 256)            15616     \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (1, None, 61)             62525     \n",
      "=================================================================\n",
      "Total params: 4,016,445\n",
      "Trainable params: 4,016,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Resolve checkpoint for re-training\n",
    "\n",
    "ckpt = tf.train.latest_checkpoint(model_path)\n",
    "print('latest checkpoint: ', ckpt)\n",
    "\n",
    "model = build_model(batch_size=1)\n",
    "model.load_weights(ckpt)\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature, n_chars=2000):\n",
    "    # Convert start string to integer vector\n",
    "    input_string = [char_to_index[char] for char in start_string]\n",
    "    input_string = tf.expand_dims(input_string, axis=0)\n",
    "    \n",
    "    generated_txt = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(n_chars):\n",
    "        predictions = model(input_string)\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, axis=0)\n",
    "        \n",
    "        # Categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        pred_id = tf.random.categorical(logits=predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # Pass predicted word as input to the model + hidden state\n",
    "        input_str = tf.expand_dims([pred_id], 0)\n",
    "        \n",
    "        generated_txt.append(idx_to_char[pred_id])\n",
    "        \n",
    "    return start_string + ''.join(generated_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_txt = generate_text(model, start_string='pop', temperature=.1, n_chars=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pop                   e     e e     e  e      e        e  e e  e                       e e  e       e     e    e             ee      e   e         ee    e    e                                    e           e  e e    e                    e e          ee                             e     e            e '"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output at loss: 2.6327\n",
    "\n",
    "generated_txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfs",
   "language": "python",
   "name": "tfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
