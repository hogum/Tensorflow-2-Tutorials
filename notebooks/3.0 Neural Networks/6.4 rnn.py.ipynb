{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character based recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1056768/1049619 [==============================] - 11s 11us/step\n"
     ]
    }
   ],
   "source": [
    "file_url = 'https://www.gutenberg.org/files/1400/1400-0.txt'\n",
    "file_path = tf.keras.utils.get_file('1400-0.txt', file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(file_path).read()\n",
    "# Strip off instruction text\n",
    "story_text = text[824:18781]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 88\n",
      " ['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ê', 'ô', '“', '”', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# get unique characters\n",
    "vocab = sorted(set(text))\n",
    "print(f'Unique characters: {len(vocab)}\\n {vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign index values to each character\n",
    "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the text to array of integers\n",
    "\n",
    "text_as_int = np.asarray([char_to_index[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1013445,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f Great Expectations\n",
      "f:62  :1 G:35 r:74 e:61 a:57 t:76  :1 E:33 x:80 p:72 e:61 c:59 t:76 a:57 t:76 i:65 o:71 n:70 s:75 "
     ]
    }
   ],
   "source": [
    "# show character mapping\n",
    "\n",
    "start, stop = 30, 50\n",
    "\n",
    "print(text[start:stop])\n",
    "for i in range(start, stop):\n",
    "    print(f'{text[i]}:{text_as_int[i]}', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare an input dataset from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Great Expectations, by Charles Dickens"
     ]
    }
   ],
   "source": [
    "# Create input Dataset\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for idx in char_dataset.take(70):\n",
    "    print(idx_to_char[idx], end= '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data for training\n",
    "\n",
    "sequence_len = 80\n",
    "examples_per_epoch = text.size[0] / sequence_len\n",
    "\n",
    "sequences = char_dataset.batch(batch_size=sequence_len + 1, drop_remainder=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfs",
   "language": "python",
   "name": "tfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
